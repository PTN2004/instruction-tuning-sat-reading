# Model configuration

base_model: "meta-llama/Llama-3.1-8B-Instruct"
# base_model: "meta-llama/Llama-3.2-1B-Instruct"
system_prompt: "You are a helpful AI assistant developed by Meta. Respond safely and accurately."

# Use quantization 4-bit NF4 for QLoRA
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

# LoRA configuration
lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
  bias: "none"
  task_type: "CAUSAL_LM"
