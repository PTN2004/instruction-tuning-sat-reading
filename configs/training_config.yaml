# Training hyperparameters

output_dir: "trained_models/sat_llama"
num_train_epochs: 2
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 2e-4
fp16: true
save_total_limit: 3
logging_steps: 10
eval_steps: 50
save_strategy: "steps"
save_steps: 50
evaluation_strategy: "steps"
optim: "paged_adamw_8bit"
lr_scheduler_type: "cosine"
warmup_ratio: 0.05
remove_unused_columns: false
report_to: "none"
load_best_model_at_end: true
metric_for_best_model: "loss"
greater_is_better: false
