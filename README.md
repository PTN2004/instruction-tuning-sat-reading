# ğŸ“š Instruction Tuning SAT Reading

This project performs **Instruction Tuning** on LLaMA/Mistral models to improve English reading comprehension, specifically for SAT Reading passages.  
The pipeline uses **LoRA/QLoRA + PEFT** to fine-tune large language models (LLMs) with instruction-style datasets.  
The system can serve inference via **FastAPI** and be deployed using Docker.

---

## ğŸš€ Features
- Fine-tune LLaMA/Mistral with **LoRA/QLoRA**
- Prepare SAT Reading datasets for instruction tuning
- Deploy inference API using **FastAPI**
- Support running on:
  - **GPU CUDA** (NVIDIA)
  - **Apple Silicon** (MPS)
  - CPU-only
- Load fine-tuned LoRA checkpoints
- Docker integration for quick deployment

---

## ğŸ“‚ Project Structure
```bash
instruction-tuning-sat-reading/
â”œâ”€â”€ configs/                  # Config files for model, training, and dataset
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â”œâ”€â”€ inference_config.yaml
â”‚   â””â”€â”€ dataset_config.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚            
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py              # FastAPI app
â”‚   â”œâ”€â”€ dataset_loader/
â”‚   â”‚   â””â”€â”€ load_dataset.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ config_loader.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_app.py
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation


### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/instruction-tuning-sat-reading.git
cd instruction-tuning-sat-reading
```

### 2ï¸âƒ£ Create virtual environment & install dependencies
**On Mac (Apple Silicon, CPU/MPS)**

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

**On GPU CUDA**

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### ğŸ”‘ Hugging Face Token Setup
1. Go to https://huggingface.co/settings/tokens

2. Create a Fine-grained token with:

    * âœ… Read access to all repos

    * âœ… Access to gated repos

3. Add it to .env file:
```ini
HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxx
```

## âš ï¸ Notes for Running on Mac
* Do not use bitsandbytes with load_in_8bit/load_in_4bit

* Use "mps" device to leverage Apple Silicon GPU

* If the model is too large â†’ consider using GGUF format + llama.cpp
